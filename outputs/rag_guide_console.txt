
[96m[1mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[96m[1mâ•‘ RAG IMPLEMENTATION GUIDE â•‘[0m
[96m[1mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


[94m[1mâ–ˆâ–ˆâ–ˆâ–ˆ What Is RAG? â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m


[96mâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
â”‚ [92m[1mComponent     [0m â”‚ [92m[1mWhat It Does                                                     [0m â”‚ [92m[1mTypical Tech                                         [0m â”‚
[96mâ”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤[0m
â”‚ Data source    â”‚ Raw documents (PDFs, web pages, knowledge-base articles, code, â€¦) â”‚ Files, S3, DB, APIs                                   â”‚
â”‚ Pre-processing â”‚ Clean, chunk, deduplicate, add metadata                           â”‚ pdfminer, beautifulsoup, nlpaug                       â”‚
â”‚ Embedding      â”‚ Turn each chunk into a dense vector (â‰ˆ768-dim)                    â”‚ Sentence-Transformers, HuggingFace text-embedding-ada â”‚
â”‚ Vector store   â”‚ Index vectors for fast similarity search                          â”‚ Pinecone, Weaviate, Qdrant, Milvus, FAISS (local)     â”‚
â”‚ Retriever      â”‚ Query â†’ top-k similar chunks (dense or hybrid)                    â”‚ BM25, DPR, ColBERT, HNSW                              â”‚
â”‚ Generator      â”‚ LLM that produces the final answer                                â”‚ GPT-4, Claude, Llama-2, Mistral, Mixtral              â”‚
â”‚ Orchestrator   â”‚ Glue: fetch chunks â†’ build prompt â†’ call LLM â†’ post-process       â”‚ LangChain, HuggingFace InstructoRAG, custom FastAPI   â”‚
â”‚ API / UI       â”‚ Expose the service to callers                                     â”‚ FastAPI, Flask, Streamlit, Gradio                     â”‚
[96mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[96mâ•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—[0m
[96mâ•‘                             ARCHITECTURE DIAGRAM                             â•‘[0m
[96mâ• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£[0m
[96mâ•‘ +-------------------+ +-------------------+ +-------------------+            â•‘[0m
[96mâ•‘ [92mData Ingestion[0m ---> [94mVector Store[0m ---> Retrieval Model      â•‘[0m
[96mâ•‘ (PDF, HTML, â€¦)     (Pinecone, â€¦)       (BM25/DPR/ColBERT)                    â•‘[0m
[96mâ•‘ +-------------------+ +-------------------+ +-------------------+            â•‘[0m
[96mâ•‘       v                                     v                                â•‘[0m
[96mâ•‘ +-------------------+ +-------------------+                                  â•‘[0m
[96mâ•‘ Embedding Service     Generator ([92mLLM[0m)                               â•‘[0m
[96mâ•‘ (Sentence-Trans.)    (GPT-4, Llama-2)                                        â•‘[0m
[96mâ•‘ +-------------------+ +-------------------+                                  â•‘[0m
[96mâ•‘       v                                     v                                â•‘[0m
[96mâ•‘ +-------------------+ +-------------------+                                  â•‘[0m
[96mâ•‘ Orchestrator ---> [93mAPI[0m / UI                                          â•‘[0m
[96mâ•‘ (LangChain)         (Fast[93mAPI[0m)                                       â•‘[0m
[96mâ•‘ +-------------------+                                                        â•‘[0m
[96mâ•‘                                                                              â•‘[0m
[96mâ•‘ +--------------------+-----------------+                                     â•‘[0m
[96mâ•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•[0m


[94m[1mâ–ˆâ–ˆâ–ˆâ–ˆ Step-by-Step Build-It-Yourself Guide â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m


[92m[1mğŸš€ STEP 1: Set Up Your Python Environment[0m
[94mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m
Create a virtual environment and install core dependencies for RAG implementation.


[48;5;236mâ”Œâ”€ PYTHON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
[48;5;236m[93m  1[0m[48;5;236m â”‚ # Create a clean venv (Python3.10+ recommended)[0m
[48;5;236m[93m  2[0m[48;5;236m â”‚ python -m venv rag-env[0m
[48;5;236m[93m  3[0m[48;5;236m â”‚ source rag-env/bin/activate[0m
[48;5;236m[93m  4[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  5[0m[48;5;236m â”‚ # Core libraries[0m
[48;5;236m[93m  6[0m[48;5;236m â”‚ pip install langchain==0.0.218[0m
[48;5;236m[93m  7[0m[48;5;236m â”‚ pip install sentence-transformers==2.5.2[0m
[48;5;236m[93m  8[0m[48;5;236m â”‚ pip install pinecone-client==0.3.0[0m
[48;5;236m[93m  9[0m[48;5;236m â”‚ pip install fastapi==0.111.0 uvicorn[standard]==0.30.1[0m
[48;5;236m[93m 10[0m[48;5;236m â”‚ pip install transformers==4.41.2[0m
[48;5;236m[93m 11[0m[48;5;236m â”‚ pip install huggingface_hub==0.24.0[0m
[48;5;236mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[46m[1mğŸ’¡ TIP:[0m [92mIf you prefer a pure-HuggingFace stack, replace langchain with llama-index or build your own orchestrator.[0m


[92m[1mâ¤ STEP 2: Gather & Pre-process Your Documents[0m
[94mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m
Load documents from various formats and chunk them for efficient processing.


[48;5;236mâ”Œâ”€ PYTHON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
[48;5;236m[93m  1[0m[48;5;236m â”‚ import os[0m
[48;5;236m[93m  2[0m[48;5;236m â”‚ import json[0m
[48;5;236m[93m  3[0m[48;5;236m â”‚ from pathlib import Path[0m
[48;5;236m[93m  4[0m[48;5;236m â”‚ import pdfminer.high_level[0m
[48;5;236m[93m  5[0m[48;5;236m â”‚ from bs4 import BeautifulSoup[0m
[48;5;236m[93m  6[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  7[0m[48;5;236m â”‚ DATA_DIR = Path("data")[0m
[48;5;236m[93m  8[0m[48;5;236m â”‚ CHUNK_SIZE = 200  # words per chunk[0m
[48;5;236m[93m  9[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 10[0m[48;5;236m â”‚ def load_text_from_pdf(file_path):[0m
[48;5;236m[93m 11[0m[48;5;236m â”‚     with open(file_path, "rb") as f:[0m
[48;5;236m[93m 12[0m[48;5;236m â”‚         return pdfminer.high_level.extract_text(f)[0m
[48;5;236m[93m 13[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 14[0m[48;5;236m â”‚ def chunk_text(text, size):[0m
[48;5;236m[93m 15[0m[48;5;236m â”‚     import re[0m
[48;5;236m[93m 16[0m[48;5;236m â”‚     sentences = re.split(r'(?<!\w\.\w.)(?<![A-Z][a-z]\.)(?<=\.|\?|\!)\s', text)[0m
[48;5;236m[93m 17[0m[48;5;236m â”‚     chunks = [][0m
[48;5;236m[93m 18[0m[48;5;236m â”‚     current_chunk = [][0m
[48;5;236m[93m 19[0m[48;5;236m â”‚     current_length = 0[0m
[48;5;236m[93m 20[0m[48;5;236m â”‚     [0m
[48;5;236m[93m 21[0m[48;5;236m â”‚     for sentence in sentences:[0m
[48;5;236m[93m 22[0m[48;5;236m â”‚         sent_length = len(sentence.split())[0m
[48;5;236m[93m 23[0m[48;5;236m â”‚         if current_length + sent_length > size and current_chunk:[0m
[48;5;236m[93m 24[0m[48;5;236m â”‚             chunks.append(" ".join(current_chunk))[0m
[48;5;236m[93m 25[0m[48;5;236m â”‚             current_chunk = [sentence][0m
[48;5;236m[93m 26[0m[48;5;236m â”‚             current_length = sent_length[0m
[48;5;236m[93m 27[0m[48;5;236m â”‚         else:[0m
[48;5;236m[93m 28[0m[48;5;236m â”‚             current_chunk.append(sentence)[0m
[48;5;236m[93m 29[0m[48;5;236m â”‚             current_length += sent_length[0m
[48;5;236m[93m 30[0m[48;5;236m â”‚     [0m
[48;5;236m[93m 31[0m[48;5;236m â”‚     if current_chunk:[0m
[48;5;236m[93m 32[0m[48;5;236m â”‚         chunks.append(" ".join(current_chunk))[0m
[48;5;236m[93m 33[0m[48;5;236m â”‚     [0m
[48;5;236m[93m 34[0m[48;5;236m â”‚     return chunks[0m
[48;5;236mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[43m[1m[91mâš ï¸  WARNING:[0m [93mLLMs have context windows (e.g., 8k tokens for Llama-2-70B). Splitting your corpus into manageable chunks lets you retrieve a few relevant passages rather than trying to fit the whole doc into the prompt.[0m


[92m[1mâ¤ STEP 3: Compute Embeddings & Store Them[0m
[94mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m
Convert text chunks to vectors and store them in a vector database for similarity search.


[94m[1mâ–ˆâ–ˆâ–ˆâ–ˆ Choose an Embedding Model â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m


[48;5;236mâ”Œâ”€ PYTHON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
[48;5;236m[93m  1[0m[48;5;236m â”‚ from sentence_transformers import SentenceTransformer[0m
[48;5;236m[93m  2[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  3[0m[48;5;236m â”‚ # Choose an embedding model[0m
[48;5;236m[93m  4[0m[48;5;236m â”‚ embedder = SentenceTransformer("all-mpnet-base-v2")  # 384-dim, good trade-off[0m
[48;5;236m[93m  5[0m[48;5;236m â”‚ # Alternative: "thenlper/gte-large" for 1024-dim, higher quality[0m
[48;5;236mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[94m[1mâ–ˆâ–ˆâ–ˆâ–ˆ Index Vectors in a Vector DB â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m


[94m[1mâ–ˆâ–ˆâ–ˆâ–ˆ Option A â€“ Pinecone (managed) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m


[48;5;236mâ”Œâ”€ PYTHON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
[48;5;236m[93m  1[0m[48;5;236m â”‚ import pinecone[0m
[48;5;236m[93m  2[0m[48;5;236m â”‚ from pinecone import IndexSettings[0m
[48;5;236m[93m  3[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  4[0m[48;5;236m â”‚ # 1ï¸âƒ£ Create the index (once)[0m
[48;5;236m[93m  5[0m[48;5;236m â”‚ PINECONE_API_KEY = "YOUR_PINECONE_API_KEY"[0m
[48;5;236m[93m  6[0m[48;5;236m â”‚ pinecone.init(api_key=PINECONE_API_KEY)[0m
[48;5;236m[93m  7[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  8[0m[48;5;236m â”‚ if not pinecone.get_index("my-rag-index"):[0m
[48;5;236m[93m  9[0m[48;5;236m â”‚     idx_settings = IndexSettings([0m
[48;5;236m[93m 10[0m[48;5;236m â”‚         name="my-rag-index",[0m
[48;5;236m[93m 11[0m[48;5;236m â”‚         dimension=384,[0m
[48;5;236m[93m 12[0m[48;5;236m â”‚         metadata_dims=16,  # optional extra metadata fields[0m
[48;5;236m[93m 13[0m[48;5;236m â”‚     )[0m
[48;5;236m[93m 14[0m[48;5;236m â”‚     pinecone.create_index(idx_settings)[0m
[48;5;236m[93m 15[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 16[0m[48;5;236m â”‚ # 2ï¸âƒ£ Insert vectors (batch)[0m
[48;5;236m[93m 17[0m[48;5;236m â”‚ def batch_upsert(chunks):[0m
[48;5;236m[93m 18[0m[48;5;236m â”‚     to_upsert = [][0m
[48;5;236m[93m 19[0m[48;5;236m â”‚     for doc in chunks:[0m
[48;5;236m[93m 20[0m[48;5;236m â”‚         embedding = embedder.encode(doc["content"], convert_to_tensor=True)[0m
[48;5;236m[93m 21[0m[48;5;236m â”‚         vector = embedding.cpu().numpy()[0m
[48;5;236m[93m 22[0m[48;5;236m â”‚         to_upsert.append({[0m
[48;5;236m[93m 23[0m[48;5;236m â”‚             "id": doc["id"],[0m
[48;5;236m[93m 24[0m[48;5;236m â”‚             "vector": vector,[0m
[48;5;236m[93m 25[0m[48;5;236m â”‚             "metadata": {[0m
[48;5;236m[93m 26[0m[48;5;236m â”‚                 "source": doc["source"],[0m
[48;5;236m[93m 27[0m[48;5;236m â”‚                 "filename": doc["metadata"]["filename"],[0m
[48;5;236m[93m 28[0m[48;5;236m â”‚                 "length": len(doc["content"].split()),[0m
[48;5;236m[93m 29[0m[48;5;236m â”‚             },[0m
[48;5;236m[93m 30[0m[48;5;236m â”‚         })[0m
[48;5;236m[93m 31[0m[48;5;236m â”‚     [0m
[48;5;236m[93m 32[0m[48;5;236m â”‚     pinecone.vectors.upsert([0m
[48;5;236m[93m 33[0m[48;5;236m â”‚         vectors=to_upsert,[0m
[48;5;236m[93m 34[0m[48;5;236m â”‚         batch_size=100  # Adjust based on your needs[0m
[48;5;236m[93m 35[0m[48;5;236m â”‚     )[0m
[48;5;236mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[46m[1mğŸ’¡ TIP:[0m [92mFor large corpora, use Pinecone's batch_upsert to send thousands of vectors in one request for better performance.[0m


[94m[1mâ–ˆâ–ˆâ–ˆâ–ˆ Option B â€“ Local FAISS (prototyping) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m


[48;5;236mâ”Œâ”€ PYTHON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
[48;5;236m[93m  1[0m[48;5;236m â”‚ import faiss[0m
[48;5;236m[93m  2[0m[48;5;236m â”‚ import numpy as np[0m
[48;5;236m[93m  3[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  4[0m[48;5;236m â”‚ # Compute all embeddings at once (memory permitting)[0m
[48;5;236m[93m  5[0m[48;5;236m â”‚ embeddings = np.stack([[0m
[48;5;236m[93m  6[0m[48;5;236m â”‚     embedder.encode(d["content"], convert_to_tensor=True).cpu().numpy()[0m
[48;5;236m[93m  7[0m[48;5;236m â”‚     for d in raw_docs[0m
[48;5;236m[93m  8[0m[48;5;236m â”‚ ])[0m
[48;5;236m[93m  9[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 10[0m[48;5;236m â”‚ # Build index (IVF + PQ works well for >10k vectors)[0m
[48;5;236m[93m 11[0m[48;5;236m â”‚ dim = embeddings.shape[1][0m
[48;5;236m[93m 12[0m[48;5;236m â”‚ index = faiss.IndexHNSWFlat(dim, 32)  # HNSW is fast for approximate nearest neighbor[0m
[48;5;236m[93m 13[0m[48;5;236m â”‚ index.hnsw.efConstruction = 200[0m
[48;5;236m[93m 14[0m[48;5;236m â”‚ index.add(embeddings)[0m
[48;5;236m[93m 15[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 16[0m[48;5;236m â”‚ # Save for later[0m
[48;5;236m[93m 17[0m[48;5;236m â”‚ faiss.write_index(index, "rag_faiss.index")[0m
[48;5;236m[93m 18[0m[48;5;236m â”‚ np.save("rag_ids.npy", ids)[0m
[48;5;236m[93m 19[0m[48;5;236m â”‚ np.save("rag_meta.npy", metadata)[0m
[48;5;236mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[92m[1mğŸ”¨ STEP 4: Build the Retriever[0m
[94mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m
Implement retrieval mechanisms to find relevant document chunks for queries.


[94m[1mâ–ˆâ–ˆâ–ˆâ–ˆ Using LangChain's PineconeRetriever â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m


[48;5;236mâ”Œâ”€ PYTHON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
[48;5;236m[93m  1[0m[48;5;236m â”‚ from langchain.vectorstores import PineconeVectorStore[0m
[48;5;236m[93m  2[0m[48;5;236m â”‚ from langchain.retrievers import PineconeRetriever[0m
[48;5;236m[93m  3[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  4[0m[48;5;236m â”‚ # Initialize the vector store[0m
[48;5;236m[93m  5[0m[48;5;236m â”‚ vector_store = PineconeVectorStore([0m
[48;5;236m[93m  6[0m[48;5;236m â”‚     index_name="my-rag-index",[0m
[48;5;236m[93m  7[0m[48;5;236m â”‚     api_key=PINECONE_API_KEY,[0m
[48;5;236m[93m  8[0m[48;5;236m â”‚ )[0m
[48;5;236m[93m  9[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 10[0m[48;5;236m â”‚ retriever = PineconeRetriever([0m
[48;5;236m[93m 11[0m[48;5;236m â”‚     vector_store=vector_store,[0m
[48;5;236m[93m 12[0m[48;5;236m â”‚     max_results=4,  # fetch 4 passages per query[0m
[48;5;236m[93m 13[0m[48;5;236m â”‚     search_type="similarity",  # dense similarity[0m
[48;5;236m[93m 14[0m[48;5;236m â”‚     include_metadata=True  # include metadata in results[0m
[48;5;236m[93m 15[0m[48;5;236m â”‚ )[0m
[48;5;236mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[94m[1mâ–ˆâ–ˆâ–ˆâ–ˆ Using a Hybrid Retriever (BM25 + Dense) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m


[48;5;236mâ”Œâ”€ PYTHON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
[48;5;236m[93m  1[0m[48;5;236m â”‚ from langchain.vectorstores import FAISSVectorStore[0m
[48;5;236m[93m  2[0m[48;5;236m â”‚ from langchain.retrievers import BM25Retriever, HybridRetriever[0m
[48;5;236m[93m  3[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  4[0m[48;5;236m â”‚ # Assume you have a FAISS index[0m
[48;5;236m[93m  5[0m[48;5;236m â”‚ faiss_store = FAISSVectorStore.read("rag_faiss.index")[0m
[48;5;236m[93m  6[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  7[0m[48;5;236m â”‚ # Sparse retriever (BM25)[0m
[48;5;236m[93m  8[0m[48;5;236m â”‚ bm25 = BM25Retriever(vector_store=faiss_store)[0m
[48;5;236m[93m  9[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 10[0m[48;5;236m â”‚ # Dense retriever (Pinecone)[0m
[48;5;236m[93m 11[0m[48;5;236m â”‚ dense = PineconeRetriever(vector_store=vector_store)[0m
[48;5;236m[93m 12[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 13[0m[48;5;236m â”‚ # Hybrid retriever (combines both)[0m
[48;5;236m[93m 14[0m[48;5;236m â”‚ hybrid = HybridRetriever([0m
[48;5;236m[93m 15[0m[48;5;236m â”‚     retrievers=[bm25, dense],[0m
[48;5;236m[93m 16[0m[48;5;236m â”‚     weights=[0.5, 0.5],  # Equal weighting[0m
[48;5;236m[93m 17[0m[48;5;236m â”‚     combine_documents=True,[0m
[48;5;236m[93m 18[0m[48;5;236m â”‚ )[0m
[48;5;236mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[46m[1mğŸ’¡ TIP:[0m [92mHybrid retrieval often provides better results than either sparse or dense retrieval alone, especially for complex queries.[0m


[92m[1mâœ… STEP 5: Choose & Load Your Generator (LLM)[0m
[94mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m
Select and initialize the Large Language Model for generating final answers.


[94m[1mâ–ˆâ–ˆâ–ˆâ–ˆ Option 1 â€“ Open-source (Llama-2-7B-Chat) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m


[48;5;236mâ”Œâ”€ PYTHON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
[48;5;236m[93m  1[0m[48;5;236m â”‚ from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline[0m
[48;5;236m[93m  2[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  3[0m[48;5;236m â”‚ model_name = "meta-llama/Llama-2-7b-chat-hf"[0m
[48;5;236m[93m  4[0m[48;5;236m â”‚ tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)[0m
[48;5;236m[93m  5[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  6[0m[48;5;236m â”‚ model = AutoModelForCausalLM.from_pretrained([0m
[48;5;236m[93m  7[0m[48;5;236m â”‚     model_name,[0m
[48;5;236m[93m  8[0m[48;5;236m â”‚     device_map="auto",  # uses GPU if available[0m
[48;5;236m[93m  9[0m[48;5;236m â”‚     torch_dtype="auto",[0m
[48;5;236m[93m 10[0m[48;5;236m â”‚ )[0m
[48;5;236m[93m 11[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 12[0m[48;5;236m â”‚ generator = pipeline([0m
[48;5;236m[93m 13[0m[48;5;236m â”‚     "text-generation",[0m
[48;5;236m[93m 14[0m[48;5;236m â”‚     model=model,[0m
[48;5;236m[93m 15[0m[48;5;236m â”‚     tokenizer=tokenizer,[0m
[48;5;236m[93m 16[0m[48;5;236m â”‚     device=0 if torch.cuda.is_available() else -1,[0m
[48;5;236m[93m 17[0m[48;5;236m â”‚     max_new_tokens=512,[0m
[48;5;236m[93m 18[0m[48;5;236m â”‚     temperature=0.7,[0m
[48;5;236m[93m 19[0m[48;5;236m â”‚     do_sample=True,[0m
[48;5;236m[93m 20[0m[48;5;236m â”‚     top_p=0.95,[0m
[48;5;236m[93m 21[0m[48;5;236m â”‚ )[0m
[48;5;236mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[94m[1mâ–ˆâ–ˆâ–ˆâ–ˆ Option 2 â€“ Commercial (GPT-4 via OpenAI) â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ[0m


[48;5;236mâ”Œâ”€ PYTHON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
[48;5;236m[93m  1[0m[48;5;236m â”‚ from langchain_community.llms import OpenAI[0m
[48;5;236m[93m  2[0m[48;5;236m â”‚ from langchain.chat_models import ChatOpenAI[0m
[48;5;236m[93m  3[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  4[0m[48;5;236m â”‚ # Using LangChain's OpenAI integration[0m
[48;5;236m[93m  5[0m[48;5;236m â”‚ llm = ChatOpenAI([0m
[48;5;236m[93m  6[0m[48;5;236m â”‚     model="gpt-4-1106-preview",[0m
[48;5;236m[93m  7[0m[48;5;236m â”‚     api_key="YOUR_OPENAI_API_KEY",[0m
[48;5;236m[93m  8[0m[48;5;236m â”‚     temperature=0.2,[0m
[48;5;236m[93m  9[0m[48;5;236m â”‚     max_tokens=512,[0m
[48;5;236m[93m 10[0m[48;5;236m â”‚     streaming=True,  # Enable token-by-token streaming[0m
[48;5;236m[93m 11[0m[48;5;236m â”‚ )[0m
[48;5;236m[93m 12[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 13[0m[48;5;236m â”‚ # Or direct OpenAI SDK[0m
[48;5;236m[93m 14[0m[48;5;236m â”‚ import openai[0m
[48;5;236m[93m 15[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 16[0m[48;5;236m â”‚ client = openai.OpenAI(api_key="YOUR_OPENAI_API_KEY")[0m
[48;5;236m[93m 17[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 18[0m[48;5;236m â”‚ def generate_with_openai(prompt):[0m
[48;5;236m[93m 19[0m[48;5;236m â”‚     response = client.chat.completions.create([0m
[48;5;236m[93m 20[0m[48;5;236m â”‚         model="gpt-4",[0m
[48;5;236m[93m 21[0m[48;5;236m â”‚         messages=[{"role": "user", "content": prompt}],[0m
[48;5;236m[93m 22[0m[48;5;236m â”‚         max_tokens=512,[0m
[48;5;236m[93m 23[0m[48;5;236m â”‚         temperature=0.2,[0m
[48;5;236m[93m 24[0m[48;5;236m â”‚     )[0m
[48;5;236m[93m 25[0m[48;5;236m â”‚     return response.choices[0].message.content[0m
[48;5;236mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[46m[1mğŸ’¡ TIP:[0m [92mLangChain treats any callable that follows the call signature (input: dict -> output: dict) as a 'callable'. This lets you swap GPT-4 for Llama-2 with a single line change.[0m


[92m[1mâ¤ STEP 6: Assemble the RAG Pipeline with LangChain[0m
[94mâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€[0m
Combine all components into a complete RAG pipeline using LangChain's built-in functionality.


[48;5;236mâ”Œâ”€ PYTHON â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”[0m
[48;5;236m[93m  1[0m[48;5;236m â”‚ from langchain.chains import RetrievalQA[0m
[48;5;236m[93m  2[0m[48;5;236m â”‚ from langchain.prompts import PromptTemplate[0m
[48;5;236m[93m  3[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  4[0m[48;5;236m â”‚ # Define a custom prompt template[0m
[48;5;236m[93m  5[0m[48;5;236m â”‚ prompt_template = '''Use the following context to answer the question at the end.[0m
[48;5;236m[93m  6[0m[48;5;236m â”‚ If you don't know the answer based on the context, just say you don't know.[0m
[48;5;236m[93m  7[0m[48;5;236m â”‚ Don't try to make up an answer.[0m
[48;5;236m[93m  8[0m[48;5;236m â”‚ [0m
[48;5;236m[93m  9[0m[48;5;236m â”‚ Context: {context}[0m
[48;5;236m[93m 10[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 11[0m[48;5;236m â”‚ Question: {question}[0m
[48;5;236m[93m 12[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 13[0m[48;5;236m â”‚ Answer:'''[0m
[48;5;236m[93m 14[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 15[0m[48;5;236m â”‚ PROMPT = PromptTemplate([0m
[48;5;236m[93m 16[0m[48;5;236m â”‚     template=prompt_template,[0m
[48;5;236m[93m 17[0m[48;5;236m â”‚     input_variables=["context", "question"][0m
[48;5;236m[93m 18[0m[48;5;236m â”‚ )[0m
[48;5;236m[93m 19[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 20[0m[48;5;236m â”‚ # Create the RAG pipeline[0m
[48;5;236m[93m 21[0m[48;5;236m â”‚ rag_pipeline = RetrievalQA.from_chain_type([0m
[48;5;236m[93m 22[0m[48;5;236m â”‚     llm=llm,  # Your chosen LLM[0m
[48;5;236m[93m 23[0m[48;5;236m â”‚     chain_type="stuff",  # Simple chain type[0m
[48;5;236m[93m 24[0m[48;5;236m â”‚     retriever=retriever,  # Your retriever[0m
[48;5;236m[93m 25[0m[48;5;236m â”‚     chain_type_kwargs={"prompt": PROMPT},[0m
[48;5;236m[93m 26[0m[48;5;236m â”‚     return_source_documents=True,  # Include source documents[0m
[48;5;236m[93m 27[0m[48;5;236m â”‚     verbose=True  # Show detailed logs[0m
[48;5;236m[93m 28[0m[48;5;236m â”‚ )[0m
[48;5;236m[93m 29[0m[48;5;236m â”‚ [0m
[48;5;236m[93m 30[0m[48;5;236m â”‚ # Use the pipeline[0m
[48;5;236m[93m 31[0m[48;5;236m â”‚ result = rag_pipeline({"query": "Your question here"})[0m
[48;5;236m[93m 32[0m[48;5;236m â”‚ print(f"Answer: {result['result']}")[0m
[48;5;236m[93m 33[0m[48;5;236m â”‚ print(f"Sources: {result['source_documents']}")[0m
[48;5;236mâ””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜[0m


[46m[1mğŸ’¡ TIP:[0m [92mLangChain ships a ready-made RAG pipeline that handles retrieval, prompt construction, LLM calling, and response parsing automatically.[0m

